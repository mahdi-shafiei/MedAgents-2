{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "medqa_test = []\n",
    "with open(\"/data/jiwoong/workspace/MedAgents-2/datasets/MedQA/50_sampled_hard_medqa/test.jsonl\", 'r') as jsfile:\n",
    "    for line in jsfile:\n",
    "        medqa_test.append(json.loads(line))\n",
    "\n",
    "queries = []\n",
    "for test in medqa_test:\n",
    "    queries.append(f\"{test['question']}\\n\\n\" + \"Options: \" + \" \".join([f\"({key}) {value}\" for key, value in test['options'].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "from pymilvus import MilvusClient\n",
    "from dotenv import load_dotenv\n",
    "import utils\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "retrieval_client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "llm_client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://azure-openai-miblab-ncu.openai.azure.com/\",\n",
    "    api_key=os.getenv(\"azure_api_key\"),\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "class CIDER:\n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.current_knowledge: List[str] = []\n",
    "        self.iteration_history: List[Dict] = []\n",
    "        self.expert_roles: List[str] = []\n",
    "        self.retrieve_topk: int = 5\n",
    "        self.rerank_topk: int = 5\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Remove markdown formatting and clean text.\"\"\"\n",
    "        return text.replace('**', '').replace(\"'''\", '').strip() if isinstance(text, str) else \"\"\n",
    "\n",
    "    def _call_llm(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Make an API call to the LLM and return the response.\"\"\"\n",
    "        response = llm_client.chat.completions.create(\n",
    "            model=self.model, messages=messages, temperature=0, max_tokens=2048\n",
    "        )\n",
    "        return self._clean_text(response.choices[0].message.content)\n",
    "\n",
    "    def process_query(self, initial_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main query processing loop.\"\"\"\n",
    "        iteration = 0\n",
    "        max_iterations = 5\n",
    "        consensus_result = \"\"\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            if iteration == 0:\n",
    "                self.expert_roles = self._generate_expert_domains(initial_query, utils.medical_specialties_gpt_selected)\n",
    "                queries = self._generate_expert_query(initial_query)\n",
    "            else:\n",
    "                follow_up_context = f\"Original Query: {initial_query} \\n\\n Previous Report: {consensus_result}\"\n",
    "                queries = self._generate_expert_query(follow_up_context)\n",
    "\n",
    "            retrieved_docs = self._retrieve_queries(queries)\n",
    "            if retrieved_docs:\n",
    "                self._update_knowledge(retrieved_docs, initial_query)\n",
    "\n",
    "            expert_analyses = self._expert_analysis(self.current_knowledge, initial_query)\n",
    "            consensus_result = self._check_consensus(initial_query, expert_analyses)\n",
    "\n",
    "            self.iteration_history.append({\n",
    "                'iteration': iteration,\n",
    "                'queries': queries,\n",
    "                'docs': retrieved_docs,\n",
    "                'analyses': expert_analyses,\n",
    "                'consensus': consensus_result\n",
    "            })\n",
    "\n",
    "            if \"consensus: yes\" in consensus_result.lower():\n",
    "                return {\n",
    "                    'final_answer': self._final_answer_pick(consensus_result),\n",
    "                    'iteration_history': self.iteration_history\n",
    "                }\n",
    "            iteration += 1\n",
    "\n",
    "        return {\n",
    "            'final_answer': self._final_answer_pick(consensus_result),\n",
    "            'iteration_history': self.iteration_history\n",
    "        }\n",
    "\n",
    "    def _update_knowledge(self, new_documents: List[str], original_query: str):\n",
    "        \"\"\"Update the current knowledge base with new documents.\"\"\"\n",
    "        combined_docs = \"\\n\".join(set(new_documents) - set(self.current_knowledge))\n",
    "        if combined_docs.strip():\n",
    "            summary = self._summarize_documents(combined_docs, original_query)\n",
    "            if summary:\n",
    "                self.current_knowledge.append(summary)\n",
    "\n",
    "    def _summarize_documents(self, documents: str, original_query: str) -> str:\n",
    "        \"\"\"Summarize the documents in relation to the original query.\"\"\"\n",
    "        summary_prompt = (\n",
    "            f\"\"\"Summarize the key insights from the following set of medical documents, considering their relevance to the original query:\n",
    "Original Query: {original_query}\n",
    "\n",
    "Documents: {documents}\n",
    "\n",
    "Please provide a concise and objective summary of the most clinically relevant information.\"\"\"\n",
    "        )\n",
    "        try:\n",
    "            return self._call_llm([\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical summarizer extracting key insights from documents.\"},\n",
    "                {\"role\": \"user\", \"content\": summary_prompt}\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing documents: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _generate_expert_domains(self, query: str, medical_fields: List[str], num_fields: int = 5) -> List[str]:\n",
    "        \"\"\"Generate relevant expert domains for the query.\"\"\"\n",
    "        question_domain_format = \"Medical Field: \" + \" | \".join([f\"Field{i}\" for i in range(num_fields)])\n",
    "        question_classifier = \"You are a medical expert who specializes in categorizing medical scenarios into specific areas of medicine.\"\n",
    "        prompt_get_question_domain = (\n",
    "            f\"Complete these steps:\\n\"\n",
    "            f\"1. Read the medical scenario: '''{query}'''.\\n\"\n",
    "            f\"2. Classify into these subfields: {', '.join(medical_fields)}.\\n\"\n",
    "            f\"3. Output exactly in this format: '''{question_domain_format}'''.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = self._clean_text(self._call_llm([\n",
    "                {\"role\": \"system\", \"content\": question_classifier},\n",
    "                {\"role\": \"user\", \"content\": prompt_get_question_domain}\n",
    "            ]))\n",
    "\n",
    "            if \"ield: \" in response:\n",
    "                domain_list = [domain.strip() for domain in response.split(\"ield: \")[1].split('|') if domain.strip()]\n",
    "                if len(domain_list) == 1:\n",
    "                    domain_list.append('General Medicine')\n",
    "                return domain_list\n",
    "            raise ValueError(\"Delimiter 'ield: ' not found in the response.\")\n",
    "        except (IndexError, ValueError, Exception) as e:\n",
    "            return ['General Medicine'] * num_fields\n",
    "\n",
    "    def _generate_expert_query(self, context: str) -> List[str]:\n",
    "        \"\"\"Generate expert queries based on context.\"\"\"\n",
    "        query_prompt = (\n",
    "            f\"\"\"Generate up to three specific medical queries addressing:\n",
    "\n",
    "{context}\n",
    "\n",
    "Consider:\n",
    "1. Expert disagreements\n",
    "2. Additional information needed\n",
    "3. Remaining knowledge gaps\n",
    "\n",
    "Format:\n",
    "1st Query: <Primary concerns>\n",
    "2nd Query: <Secondary aspects>\n",
    "3rd Query: <Remaining gaps>\n",
    "\n",
    "Make queries specific and targeted.\"\"\"\n",
    "        )\n",
    "\n",
    "        all_queries = []\n",
    "        for role in self.expert_roles:\n",
    "            try:\n",
    "                response = self._call_llm([\n",
    "                    {\"role\": \"system\", \"content\": f\"You are a medical expert in {role}.\"},\n",
    "                    {\"role\": \"user\", \"content\": query_prompt.format(context=context)}\n",
    "                ])\n",
    "                queries = [\n",
    "                    line.split(\":\")[-1].strip() for line in response.split(\"\\n\")\n",
    "                    if line.strip().startswith((\"1st Query\", \"2nd Query\", \"3rd Query\"))\n",
    "                ]\n",
    "                all_queries.extend(queries)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating queries for {role}: {str(e)}\")\n",
    "        return all_queries\n",
    "\n",
    "    def _retrieve_queries(self, queries: List[str]) -> List[str]:\n",
    "        \"\"\"Retrieve documents based on queries.\"\"\"\n",
    "        retrieved_docs = []\n",
    "        for query in queries:\n",
    "            try:\n",
    "                docs = utils.rerank(query, utils.retrieve(query, retrieval_client, topk=self.retrieve_topk))\n",
    "                retrieved_docs.extend(docs[:self.rerank_topk])\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving documents for query '{query}': {str(e)}\")\n",
    "\n",
    "        # Remove duplicates while maintaining order\n",
    "        seen = set()\n",
    "        return [x for x in retrieved_docs if not (x in seen or seen.add(x))]\n",
    "\n",
    "    def _expert_analysis(self, documents_or_knowledgebase: List[str], query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Perform expert analysis on the current knowledge base.\"\"\"\n",
    "        analysis_prompt = (\n",
    "            f\"\"\"Analyze this knowledge base and solve the query:\n",
    "\n",
    "Current Knowledge: {self._format_docs_for_prompt(documents_or_knowledgebase)}\n",
    "\n",
    "Original Query: {query}\n",
    "\n",
    "Provide:\n",
    "1. Key Information: <Critical information>\n",
    "2. Remaining Questions: <Gaps to address>\n",
    "3. Reasoning: <Justification>\n",
    "4. Answer: <Conclude your response with the phrase \\\"the answer is ([option_id]) [answer_string]\\\">\"\"\"\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                'role': role,\n",
    "                'analysis': self._call_llm([\n",
    "                    {\"role\": \"system\", \"content\": f\"You are a {role} specialist analyzing medical information.\"},\n",
    "                    {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "                ])\n",
    "            }\n",
    "            for role in self.expert_roles\n",
    "        ]\n",
    "\n",
    "    def _check_consensus(self, query: str, expert_analyses: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Check if there is a consensus among expert analyses.\"\"\"\n",
    "        consensus_prompt = (\n",
    "            \"\"\"Review the expert opinions below and determine whether they reach a consensus.\n",
    "\n",
    "Instructions:\n",
    "1. Indicate if there is a consensus: <yes/no>\n",
    "2. If there is a consensus, provide the agreed-upon answer choice: <state the answer>\n",
    "3. If there is no consensus, describe the disagreements.\n",
    "\n",
    "Expert Analyses:\n",
    "{}\n",
    "\n",
    "Original Query:\n",
    "{}\n",
    "\"\"\".format(''.join([f\"Expert ({a['role']}):\\n{a['analysis']}\\n\\n\" for a in expert_analyses]), query)\n",
    "        )\n",
    "        return self._call_llm([\n",
    "            {\"role\": \"system\", \"content\": \"You are evaluating expert opinions for consensus on a question.\"},\n",
    "            {\"role\": \"user\", \"content\": consensus_prompt}\n",
    "        ])\n",
    "\n",
    "    def _format_docs_for_prompt(self, documents: List[str]) -> str:\n",
    "        \"\"\"Format documents for inclusion in prompts.\"\"\"\n",
    "        return \"\\n\\n\".join([f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(documents)])\n",
    "\n",
    "    def _final_answer_pick(self, text: str) -> str:\n",
    "        \"\"\"Pick the final answer based on the consensus text.\"\"\"\n",
    "        return self._call_llm([\n",
    "            {\"role\": \"system\", \"content\": \"You are an answer parser. Pick an answer even if there is no consensus.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Only output A, B, C, or D from {text}\"}\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:41<00:00, 24.83s/it]\n"
     ]
    }
   ],
   "source": [
    "cider_instance = CIDER()\n",
    "\n",
    "def process_queries_with_threadpool(cider_instance, queries):\n",
    "    result = [None] * len(queries)  # 결과 리스트 생성 (입력 순서 유지)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # 각 작업을 제출하고, (인덱스, Future) 쌍으로 저장\n",
    "        futures = {executor.submit(cider_instance.process_query, query): idx for idx, query in enumerate(queries)}\n",
    "        \n",
    "        # tqdm을 사용하여 진행률 표시줄 생성 및 작업 진행 상태 추적\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            idx = futures[future]  # 인덱스 가져오기\n",
    "            result[idx] = future.result()  # 결과 저장\n",
    "\n",
    "    return result\n",
    "\n",
    "# 결과 처리 및 출력\n",
    "result = process_queries_with_threadpool(cider_instance, queries[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/jiwoong/workspace/output/v5\", 'w') as jsfile:\n",
    "    json.dump(result, jsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cider_instance = CIDER(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = medqa_test[122]\n",
    "query = f\"{test['question']}\\n\\n\" + \"Options: \" + \" \".join([f\"({key}) {value}\" for key, value in test['options'].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A 28-year-old gravida 1 at 32 weeks gestation is evaluated for an abnormal ultrasound that showed fetal microcephaly. Early in the 1st trimester, she had fevers and headaches for 1 week. She also experienced myalgias, arthralgias, and a pruritic maculopapular rash. The symptoms resolved without any medications. A week prior to her symptoms, she had traveled to Brazil where she spent most of the evenings hiking. She did not use any mosquito repellents. There is no personal or family history of chronic or congenital diseases. Medications include iron supplementation and a multivitamin. She received all of the recommended childhood vaccinations. She does not drink alcohol or smoke cigarettes. The IgM and IgG titers for toxoplasmosis were negative. Which of the following is the most likely etiologic agent?\\n\\nOptions: (A) Dengue virus (B) Rubella virus (C) Toxoplasmosis (D) Zika virus'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b91f4dca0940d59448a1633cd66b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Queries:   0%|          | 0/1273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "difficulty_list = [None] * len(medqa_test)  # Pre-allocate the list to maintain order\n",
    "\n",
    "def assess_difficulty(index, test):\n",
    "    query = f\"{test['question']}\\n\\n\" + \"Options: \" + \" \".join([f\"({key}) {value}\" for key, value in test['options'].items()])\n",
    "    return index, cider_instance.assess_query_difficulty(query)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(assess_difficulty, i, test) for i, test in enumerate(medqa_test)]\n",
    "    \n",
    "    for future in tqdm(futures, total=len(medqa_test), desc=\"Processing Queries\"):\n",
    "        index, result = future.result()\n",
    "        difficulty_list[index] = result  # Store the result at the correct index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficulty_list.count('easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_direct = []\n",
    "with open(\"/data/jiwoong/workspace/MedAgents-2/baselines/MedAgents/outputs/MedQA/gpt4omini-base_direct\", 'r') as jsfile:\n",
    "    for line in jsfile:\n",
    "        base_direct.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8368700265251989\n",
      "0.7341040462427746\n"
     ]
    }
   ],
   "source": [
    "easy=0\n",
    "hard=0\n",
    "for i in range(1273):\n",
    "    if difficulty_list[i] == 'easy' and base_direct[i]['pred_answer'] == base_direct[i]['gold_answer']:\n",
    "        easy += 1\n",
    "    elif difficulty_list[i] == 'hard' and base_direct[i]['pred_answer'] == base_direct[i]['gold_answer']:\n",
    "        hard += 1\n",
    "print(easy/difficulty_list.count('easy'))\n",
    "print(hard/difficulty_list.count('hard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
