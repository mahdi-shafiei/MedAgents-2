{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from openai import AzureOpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import utils\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "retrieval_client = MilvusClient(\n",
    "    uri=\"http://localhost:19530\"\n",
    ")\n",
    "\n",
    "#llm_client = OpenAI(\n",
    "#    api_key=OPENAPI_API_KEY,\n",
    "#)\n",
    "\n",
    "llm_client = AzureOpenAI(\n",
    "    azure_endpoint = \"https://azure-openai-miblab-ncu.openai.azure.com/\",\n",
    "    api_key = os.getenv(\"azure_api_key\"),\n",
    "    api_version = \"2024-08-01-preview\",\n",
    ")\n",
    "\n",
    "class CIDER:\n",
    "    def __init__(self, model=\"gpt-4o-mini\"):\n",
    "\n",
    "        self.model = model\n",
    "        self.current_knowledge = []\n",
    "        self.iteration_history = []\n",
    "        self.expert_roles = []\n",
    "        \n",
    "    def process_query(self, initial_query: str) -> Dict[str, Any]:\n",
    "        iteration = 0\n",
    "        consensus_reached = False\n",
    "        \n",
    "        while not consensus_reached:\n",
    "            print(f\"Starting iteration {iteration}\")\n",
    "            \n",
    "            # 0. Domain Expert Generation\n",
    "            expert_roles = self._generate_expert_domains(initial_query)\n",
    "\n",
    "            # 1. Query Generation\n",
    "            queries = self._generate_expert_query(initial_query)\n",
    "            \n",
    "            # 2. Document Retrieval\n",
    "            retrieved_docs = []\n",
    "            for query in expert_queries:\n",
    "                docs = utils.retrieve(query)\n",
    "                docs = utils.rerank(query, docs)\n",
    "                retrieved_docs.extend(docs)\n",
    "            \n",
    "            # 3. Expert Analysis\n",
    "            expert_analyses = self._expert_analysis(retrieved_docs)\n",
    "            \n",
    "            # 4. Consensus Check\n",
    "            consensus_result = self._check_consensus(expert_analyses)\n",
    "            consensus_reached = consensus_result['consensus_reached']\n",
    "            \n",
    "            # 5. Record Iteration\n",
    "            self.iteration_history.append({\n",
    "                'iteration': iteration,\n",
    "                'queries': expert_queries,\n",
    "                'docs': retrieved_docs,\n",
    "                'analyses': expert_analyses,\n",
    "                'consensus': consensus_result\n",
    "            })\n",
    "            \n",
    "            # 6. Update Knowledge\n",
    "            self.current_knowledge.extend(retrieved_docs)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "        return self._generate_final_report()\n",
    "\n",
    "    def _generate_expert_domains(self, query: str) -> List[str]:\n",
    "       domain_prompt = \"\"\"Given the medical query below, identify most relevant medical specialties needed for comprehensive analysis.\n",
    "    \n",
    "       Query: {query}\n",
    "    \n",
    "       List only the specialties, one per line.\"\"\"\n",
    "    \n",
    "       messages = [\n",
    "           {\"role\": \"system\", \"content\": \"You are a medical domain expert selector.\"},\n",
    "           {\"role\": \"user\", \"content\": domain_prompt.format(query=query)}\n",
    "       ]\n",
    "    \n",
    "       try:\n",
    "           response = self._call_llm(messages)\n",
    "           domains = [domain.strip() for domain in response.split('\\n') if domain.strip()]\n",
    "\n",
    "           # Generate expert roles for each domain\n",
    "           expert_roles = []\n",
    "           for domain in domains:\n",
    "               role = f\"\"\"You are a medical expert specialized in {domain}. \n",
    "               Analyze information through the lens of your specialty.\n",
    "               Focus on aspects relevant to {domain}.\n",
    "               Identify gaps in domain-specific knowledge.\"\"\"\n",
    "               expert_roles.append(role)\n",
    "\n",
    "           return expert_roles\n",
    "\n",
    "       except Exception as e:\n",
    "           print(f\"Error generating expert domains: {str(e)}\")\n",
    "           # Fallback to core specialties\n",
    "           return [\n",
    "               \"You are an internal medicine specialist focusing on overall case integration.\",\n",
    "               \"You are a diagnostic specialist focusing on differential diagnosis.\",\n",
    "               \"You are a clinical specialist focusing on treatment approaches.\"\n",
    "           ]\n",
    "\n",
    "\n",
    "    def _generate_expert_query(self, context: str) -> List[str]:\n",
    "        all_queries = []\n",
    "\n",
    "        query_prompt = \"\"\"Analyze the medical context and generate a snippet of medical document to fill knowledge gaps.\n",
    "\n",
    "    Context: {context}\n",
    "    Knowledge Gaps: {gaps}\n",
    "\n",
    "    Instructions:\n",
    "    - Generate one specific medical document addressing the knowledge gaps\n",
    "    - Focus on gathering concrete, clinically relevant information\n",
    "    - Ensure each query is distinct and detailed\n",
    "\n",
    "    Queries:\"\"\"\n",
    "\n",
    "        for role in expert_roles:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a medical expert specialized in the {op_domain} domain generating clinically relevant documents.\"},\n",
    "                {\"role\": \"user\", \"content\": query_prompt.format(\n",
    "                    context=context,\n",
    "                    gaps=self._identify_knowledge_gaps()\n",
    "                )}\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                response = self._call_llm(messages)\n",
    "                queries = response.split[\"Queries:\"][-1]\n",
    "                all_queries.extend(queries)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating queries for role {role}: {str(e)}\")\n",
    "                continue\n",
    "        return list(all_queries)\n",
    "\n",
    "    def _expert_analysis(self, documents, context) -> List[Dict]:\n",
    "       analyses = []\n",
    "    \n",
    "       analysis_prompt = \"\"\"Given the medical context below, analyze the following documents based on your expertise.\n",
    "    \n",
    "       Original Context:\n",
    "       {context}\n",
    "\n",
    "       Retrieved Documents:\n",
    "       {docs}\n",
    "    \n",
    "       Provide your analysis in the following format:\n",
    "       1. Key Findings (specifically relating to the original query):\n",
    "       2. Remaining Questions (what else do we need to know to fully address the query):\n",
    "       3. Is the information sufficient to answer the original query? (yes/no)\n",
    "       4. Explain your reasoning for why the information is or isn't sufficient.\"\"\"\n",
    "    \n",
    "       for role in expert_roles:\n",
    "           messages = [\n",
    "               {\"role\": \"system\", \"content\": role},\n",
    "               {\"role\": \"user\", \"content\": analysis_prompt.format(\n",
    "                   context=query_context,\n",
    "                   docs=self._format_docs_for_prompt(documents)\n",
    "               )}\n",
    "           ]\n",
    "\n",
    "           response = self._call_llm(messages)\n",
    "           analyses.append({\n",
    "               'role': role,\n",
    "               'analysis': response\n",
    "           })\n",
    "\n",
    "       return analyses\n",
    "\n",
    "    def _check_consensus(self, expert_analyses: List[Dict]) -> Dict[str, Any]:\n",
    "        consensus_prompt = \"\"\"Review the analyses from all experts and determine if consensus has been reached.\n",
    "        \n",
    "        Expert Analyses:\n",
    "        {analyses}\n",
    "        \n",
    "        Determine:\n",
    "        1. Is there consensus? (yes/no)\n",
    "        2. What are the remaining disagreements?\n",
    "        3. What additional information is needed?\"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical consensus evaluator.\"},\n",
    "            {\"role\": \"user\", \"content\": consensus_prompt.format(\n",
    "                analyses=self._format_analyses_for_prompt(expert_analyses)\n",
    "            )}\n",
    "        ]\n",
    "        \n",
    "        response = self._call_llm(messages)\n",
    "        \n",
    "        # Parse response to determine consensus\n",
    "        consensus_reached = 'yes' in response.lower().split('\\n')[0]\n",
    "        \n",
    "        return {\n",
    "            'consensus_reached': consensus_reached,\n",
    "            'full_evaluation': response\n",
    "        }\n",
    "\n",
    "    def _call_llm(self, messages: List[Dict]) -> str:\n",
    "        response = llm_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def _identify_knowledge_gaps(self) -> str:\n",
    "        \"\"\"Identify current knowledge gaps from iteration history.\"\"\"\n",
    "        if not self.iteration_history:\n",
    "            return \"Initial query - no current gaps identified\"\n",
    "            \n",
    "        last_iteration = self.iteration_history[-1]\n",
    "        gaps = []\n",
    "        \n",
    "        for analysis in last_iteration['analyses']:\n",
    "            if 'Remaining Questions' in analysis['analysis']:\n",
    "                questions = analysis['analysis'].split('Remaining Questions:')[1].split('\\n')\n",
    "                gaps.extend([q.strip() for q in questions if q.strip()])\n",
    "                \n",
    "        return '\\n'.join(gaps)\n",
    "\n",
    "    def _format_docs_for_prompt(self, docs) -> str:\n",
    "#        formatted_docs = []\n",
    "#        for i, doc in enumerate(docs[:], 1):\n",
    "#            formatted_docs.append(f\"Document {i}:\\n{doc['content'][:500]}...\")\n",
    "        return '\\n'.join(docs)\n",
    "\n",
    "    def _format_analyses_for_prompt(self, analyses: List[Dict]) -> str:\n",
    "        formatted = []\n",
    "        for analysis in analyses:\n",
    "            formatted.append(f\"Expert ({analysis['role']}):\\n{analysis['analysis']}\")\n",
    "        return '\\n\\n'.join(formatted)\n",
    "\n",
    "    def _generate_final_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate final report with findings and metadata.\"\"\"\n",
    "        report_prompt = \"\"\"Generate a final medical report based on all gathered information.\n",
    "        \n",
    "        Iteration History:\n",
    "        {history}\n",
    "        \n",
    "        Format the report with:\n",
    "        1. Key Findings\n",
    "        2. Recommendations\n",
    "        3. Confidence Level\n",
    "        4. References to Key Documents\"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical report synthesizer.\"},\n",
    "            {\"role\": \"user\", \"content\": report_prompt.format(\n",
    "                history=self._format_history_for_prompt()\n",
    "            )}\n",
    "        ]\n",
    "        \n",
    "        report = self._call_llm(messages)\n",
    "        \n",
    "        return {\n",
    "            'report': report,\n",
    "            'iterations': len(self.iteration_history),\n",
    "            'total_documents': len(self.current_knowledge)\n",
    "        }\n",
    "\n",
    "    def _format_history_for_prompt(self) -> str:\n",
    "        \"\"\"Format iteration history for final report.\"\"\"\n",
    "        formatted = []\n",
    "        for iteration in self.iteration_history:\n",
    "            formatted.append(f\"\"\"\n",
    "            Iteration {iteration['iteration']}:\n",
    "            - Queries Asked: {len(iteration['queries'])}\n",
    "            - Documents Retrieved: {len(iteration['docs'])}\n",
    "            - Expert Analyses: {len(iteration['analyses'])}\n",
    "            \"\"\")\n",
    "        return '\\n'.join(formatted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
