{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "from pymilvus import MilvusClient\n",
    "from dotenv import load_dotenv\n",
    "import utils\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "retrieval_client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "llm_client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://azure-openai-miblab-ncu.openai.azure.com/\",\n",
    "    api_key=os.getenv(\"azure_api_key\"),\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "class CIDER:\n",
    "    def __init__(self, model=\"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.current_knowledge = []\n",
    "        self.iteration_history = []\n",
    "        self.expert_roles = []\n",
    "        self.retrieve_topk = 100\n",
    "        self.rerank_topk = 5\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Remove markdown formatting and clean text.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        return text.replace('**', '').replace(\"\\'\\'\\'\", '').strip()\n",
    "\n",
    "    def _call_llm(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Make an API call to the LLM and return the response.\"\"\"\n",
    "        response = llm_client.chat.completions.create(\n",
    "            model=self.model, messages=messages, temperature=0, max_tokens=2048\n",
    "        )\n",
    "#        print(\"\\n\" + \"=\"*50 + \" LLM Input Messages \" + \"=\"*50)\n",
    "#        for message in messages:\n",
    "#            print(f\"{message['role']}: {message['content']}\\n\")\n",
    "        output = response.choices[0].message.content\n",
    "#        print(\"\\n\" + \"=\"*50 + \" LLM Output Response \" + \"=\"*50)\n",
    "#        print(output + \"\\n\")\n",
    "        return self._clean_text(output)\n",
    "\n",
    "\n",
    "    def process_query(self, initial_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main query processing loop.\"\"\"\n",
    "        iteration = 0\n",
    "        max_iterations = 5\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            if iteration == 0:\n",
    "                self.expert_roles = self._generate_expert_domains(initial_query, utils.medical_specialties_gpt_selected)\n",
    "                queries = self._generate_expert_query(initial_query)\n",
    "            else:\n",
    "                follow_up_context = f\"Original Query: {initial_query} \\n\\n Previous Report: {consensus_result}\"\n",
    "                queries = self._generate_expert_query(follow_up_context)\n",
    "\n",
    "            retrieved_docs = self._retrieve_queries(queries)\n",
    "            \n",
    "            if retrieved_docs:\n",
    "                self._update_knowledge(retrieved_docs, initial_query)\n",
    "\n",
    "            expert_analyses = self._expert_analysis(self.current_knowledge, initial_query)\n",
    "            #expert_analyses.append(self._cot_generalmedicine(initial_query))\n",
    "            consensus_result = self._check_consensus(initial_query, expert_analyses, )\n",
    "\n",
    "            self.iteration_history.append({\n",
    "                'iteration': iteration,\n",
    "                'queries': queries,\n",
    "                'docs': retrieved_docs,\n",
    "                'analyses': expert_analyses,\n",
    "                'consensus': consensus_result\n",
    "            })\n",
    "\n",
    "            if \"ensus: yes\" in consensus_result.lower():\n",
    "                return self._final_answer_pick(consensus_result), self.iteration_history\n",
    "            iteration += 1\n",
    "\n",
    "        return self._final_answer_pick(consensus_result), self.iteration_history\n",
    "\n",
    "    def _update_knowledge(self, new_documents: List[str], original_query: str):\n",
    "        \"\"\"Update the current knowledge base with new documents.\"\"\"\n",
    "        combined_docs = \"\\n\".join(set(new_documents) - set(self.current_knowledge))\n",
    "        if combined_docs.strip():\n",
    "            summary = self._summarize_documents(combined_docs, original_query)\n",
    "            if summary:\n",
    "                self.current_knowledge.append(summary)\n",
    "\n",
    "    def _summarize_documents(self, documents: str, original_query: str) -> str:\n",
    "        summary_prompt = f\"\"\"Summarize the key insights from the following set of medical documents, considering their relevance to the original query:\n",
    "\n",
    "        Original Query: {original_query}\n",
    "\n",
    "        Documents: {documents}\n",
    "\n",
    "        Please provide a concise and objective summary of the most clinically relevant information. Use exact phrases from the documents where appropriate, and do not suggest or conclude any answer choice. Focus on explaining relevant mechanisms, key facts, and insights that address the query.\"\"\"\n",
    "        try:\n",
    "            return self._clean_text(self._call_llm([\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical summarizer extracting key insights from documents.\"},\n",
    "                {\"role\": \"user\", \"content\": summary_prompt}\n",
    "            ]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing documents: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def assess_query_difficulty(self, query: str) -> str:\n",
    "        \"\"\"Assess the complexity of a medical query.\"\"\"\n",
    "        difficulty_prompt = \"\"\"Given the medical query below, assess its difficulty:\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Options:\n",
    "        - Easy: A single medical agent can answer it.\n",
    "        - Hard: A multi-agent system is needed.\n",
    "\n",
    "        Format your response:\n",
    "        Explanation: <Brief explanation>\n",
    "        Difficulty: <easy/hard>\"\"\"\n",
    "\n",
    "        response = self._call_llm([\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical expert evaluating query complexity.\"},\n",
    "            {\"role\": \"user\", \"content\": difficulty_prompt.format(query=query)}\n",
    "        ])\n",
    "        return 'hard' if 'hard' in response.lower() else 'easy'\n",
    "\n",
    "    def _generate_expert_domains(self, query: str, medical_fields: List[str], num_fields: int = 5) -> List[str]:\n",
    "        \"\"\"Generate relevant expert domains for the query.\"\"\"\n",
    "        question_domain_format = \"Medical Field: \" + \" | \".join([f\"Field{i}\" for i in range(num_fields)])\n",
    "        question_classifier = \"You are a medical expert who specializes in categorizing medical scenarios into specific areas of medicine.\"\n",
    "        prompt_get_question_domain = (\n",
    "            f\"Complete these steps:\\n\"\n",
    "            f\"1. Read the medical scenario: '''{query}'''.\\n\"\n",
    "            f\"2. Classify into these subfields: {', '.join(medical_fields)}.\\n\"\n",
    "            f\"3. Output exactly in this format: '''{question_domain_format}'''.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = self._clean_text(self._call_llm([\n",
    "                {\"role\": \"system\", \"content\": question_classifier},\n",
    "                {\"role\": \"user\", \"content\": prompt_get_question_domain}\n",
    "            ]))\n",
    "\n",
    "            if \"ield: \" in response:\n",
    "                domain_list = [domain.strip() for domain in response.split(\"ield: \")[1].split('|') if domain.strip()]\n",
    "                if len(domain_list) == 1:\n",
    "                    domain_list.append('General Medicine')\n",
    "                return domain_list\n",
    "            raise ValueError(\"Delimiter 'ield: ' not found in the response.\")\n",
    "        except (IndexError, ValueError, Exception) as e:\n",
    "            return ['General Medicine'] * num_fields\n",
    "\n",
    "    def _generate_expert_query(self, context: str) -> List[str]:\n",
    "        \"\"\"Generate expert queries based on context.\"\"\"\n",
    "        query_prompt = \"\"\"Generate up to three specific medical queries addressing:\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Consider:\n",
    "        1. Expert disagreements\n",
    "        2. Additional information needed\n",
    "        3. Remaining knowledge gaps\n",
    "\n",
    "        Format:\n",
    "        1st Query: <Primary concerns>\n",
    "        2nd Query: <Secondary aspects>\n",
    "        3rd Query: <Remaining gaps>\n",
    "\n",
    "        Make queries specific and targeted.\"\"\"\n",
    "\n",
    "        all_queries = []\n",
    "        for role in self.expert_roles:\n",
    "            try:\n",
    "                response = self._clean_text(self._call_llm([\n",
    "                    {\"role\": \"system\", \"content\": f\"You are a medical expert in {role}.\"},\n",
    "                    {\"role\": \"user\", \"content\": query_prompt.format(context=context)}\n",
    "                ]))\n",
    "                queries = [\n",
    "                    line.split(\":\")[-1].strip() for line in response.split(\"\\n\")\n",
    "                    if line.strip().startswith((\"1st Query\", \"2nd Query\", \"3rd Query\"))\n",
    "                ]\n",
    "                all_queries.extend(queries)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating queries for {role}: {str(e)}\")\n",
    "        return all_queries[:]\n",
    "\n",
    "    def _expert_analysis(self, documents_or_knowledgebase: List[str], query: str) -> List[Dict]:\n",
    "        \"\"\"Perform expert analysis on the current knowledge base.\"\"\"\n",
    "        analysis_prompt = f\"\"\"Analyze this knowledge base and solve the query:\n",
    "\n",
    "        Current Knowledge: {self._format_docs_for_prompt(documents_or_knowledgebase)}\n",
    "\n",
    "        Original Query: {query}\n",
    "\n",
    "        Provide:\n",
    "        1. Key Information: <Critical information>\n",
    "        2. Remaining Questions: <Gaps to address>\n",
    "        3. Reasoning: <Justification>\n",
    "        4. Answer: <You are strongly required to follow the specified output format; conclude your response with the phrase \\\"the answer is ([option_id]) [answer_string]\\\", \"\"\"\n",
    "\n",
    "        return [{\n",
    "            'role': role,\n",
    "            'analysis': self._clean_text(self._call_llm([\n",
    "                {\"role\": \"system\", \"content\": f\"You are a {role} specialist analyzing medical information.\"},\n",
    "                {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "            ]))\n",
    "        } for role in self.expert_roles]\n",
    "\n",
    "    def _cot_generalmedicine(self, query: str) -> Dict:\n",
    "        return {\n",
    "            'role': \"General Medicine\",\n",
    "            'analysis': self._clean_text(self._call_llm([\n",
    "                {\"role\": \"system\", \"content\": \"The following is a multiple-choice question about medical knowledge. Solve this in a step-by-step fashion, starting by summarizing the available information. Output a single option from the given options as the final answer. You are strongly required to follow the specified output format; conclude your response with the phrase \\\"the answer is ([option_id]) [answer_string]\\\"\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {query}\"}\n",
    "            ]))\n",
    "        }\n",
    "\n",
    "    def _retrieve_queries(self, queries: List[str]) -> List[str]:\n",
    "        retrieved_docs = []\n",
    "        for query in queries:\n",
    "            docs = utils.rerank(query, utils.retrieve(query, retrieval_client, topk = self.retrieve_topk))\n",
    "            retrieved_docs.extend(docs[:self.rerank_topk])\n",
    "        seen = set()\n",
    "        seen_add = seen.add\n",
    "        return [x for x in retrieved_docs if not (x in seen or seen_add(x))]\n",
    "\n",
    "    def _check_consensus(self, query, expert_analyses: List[Dict]) -> str:\n",
    "        consensus_prompt = \"\"\"Review the expert opinions below and determine whether they reach a consensus.\n",
    "\n",
    "        Instructions:\n",
    "        1. Indicate if there is a consensus: <yes/no>  (Consensus is achieved if ALL experts agree on the same answer choice, regardless of whether the answer is correct.)\n",
    "        2. If there is a consensus, provide the agreed-upon answer choice: <state the answer>\n",
    "        3. If there is no consensus, describe the disagreements and suggest areas to search for more information to clarify the correct answer.\n",
    "\n",
    "        Expert Analyses:\n",
    "        {a}\n",
    "\n",
    "        Original Query:\n",
    "        {q}\n",
    "        \n",
    "        Please follow the output format:\n",
    "\n",
    "        1. Consensus: yes or no\n",
    "        2. if answer, \n",
    "            The answer is: (A), (B), (C), or (D) answer\n",
    "            if no consensus,\n",
    "            Disagreements: detailed disagreements\n",
    "        \"\"\"\n",
    "\n",
    "        response = self._clean_text(self._call_llm([\n",
    "            {\"role\": \"system\", \"content\": \"You are evaluating expert opinions for consensus on a question.\"},\n",
    "            {\"role\": \"user\", \"content\": consensus_prompt.format(\n",
    "                a='\\n\\n'.join([f\"Expert ({a['role']}):\\n{a['analysis']}\" for a in expert_analyses]),\n",
    "                q=query\n",
    "            )}\n",
    "        ]))\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _identify_knowledge_gaps(self) -> str:\n",
    "        \"\"\"Identify knowledge gaps from the last iteration.\"\"\"\n",
    "        if not self.iteration_history:\n",
    "            return \"Initial query - no gaps identified\"\n",
    "\n",
    "        last_iteration = self.iteration_history[-1]\n",
    "        gaps = []\n",
    "        for analysis in last_iteration['analyses']:\n",
    "            try:\n",
    "                if analysis.get('analysis') and 'Remaining Questions' in analysis['analysis']:\n",
    "                    questions_part = analysis['analysis'].split('Remaining Questions:')\n",
    "                    if len(questions_part) > 1:\n",
    "                        questions = [\n",
    "                            self._clean_text(q)\n",
    "                            for q in questions_part[1].split('\\n') \n",
    "                            if self._clean_text(q)\n",
    "                        ]\n",
    "                        gaps.extend(questions)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing analysis for gaps: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "        return '\\n'.join(gaps) if gaps else \"No specific gaps identified\"\n",
    "\n",
    "    def _format_docs_for_prompt(self, documents: List[str]) -> str:\n",
    "        \"\"\"Format documents for inclusion in prompts.\"\"\"\n",
    "        return \"\\n\\n\".join([f\"Document {i+1}:\\n{self._clean_text(doc)}\" for i, doc in enumerate(documents)])\n",
    "\n",
    "    def _final_answer_pick(self, text):\n",
    "        return self._call_llm([\n",
    "                    {\"role\": \"system\", \"content\": f\"You are a answer parser. Pick an answer even if there is no consensus.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Only output A, B, C, or D from {text}\"}\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "medqa_test = []\n",
    "with open(\"/data/jiwoong/workspace/MedAgents-2/datasets/MedQA/50_sampled_hard_medqa/test.jsonl\", 'r') as jsfile:\n",
    "    for line in jsfile:\n",
    "        medqa_test.append(json.loads(line))\n",
    "\n",
    "queries = []\n",
    "for test in medqa_test:\n",
    "    queries.append(f\"{test['question']}\\n\\n\" + \"Options: \" + \" \".join([f\"({key}) {value}\" for key, value in test['options'].items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:21<17:55, 21.96s/it]"
     ]
    }
   ],
   "source": [
    "cider_instance = CIDER()\n",
    "\n",
    "def process_queries_with_threadpool(cider_instance, queries):\n",
    "    result = [None] * len(queries)  # 결과 리스트 생성 (입력 순서 유지)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # 각 작업을 제출하고, (인덱스, Future) 쌍으로 저장\n",
    "        futures = {executor.submit(cider_instance.process_query, query): idx for idx, query in enumerate(queries)}\n",
    "        \n",
    "        # tqdm을 사용하여 진행률 표시줄 생성 및 작업 진행 상태 추적\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            idx = futures[future]  # 인덱스 가져오기\n",
    "            result[idx] = future.result()  # 결과 저장\n",
    "\n",
    "    return result\n",
    "\n",
    "# 결과 처리 및 출력\n",
    "result = process_queries_with_threadpool(cider_instance, queries[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "A\n",
      "A\n",
      "B\n",
      "D\n",
      "C\n",
      "A\n",
      "A\n",
      "B\n",
      "B\n",
      "A\n",
      "B\n",
      "A\n",
      "D\n",
      "C\n",
      "C\n",
      "A\n",
      "A\n",
      "A\n",
      "C\n",
      "B\n",
      "A\n",
      "D\n",
      "A\n",
      "A\n",
      "C\n",
      "C\n",
      "D\n",
      "B\n",
      "B\n",
      "D\n",
      "B\n",
      "A\n",
      "B\n",
      "C\n",
      "C\n",
      "A\n",
      "A\n",
      "B\n",
      "D\n",
      "D\n",
      "A\n",
      "A\n",
      "D\n",
      "D\n",
      "D\n",
      "C\n",
      "A\n",
      "B\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "for i in result:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/jiwoong/workspace/output/v4\", 'w') as jsfile:\n",
    "    json.dump(result, jsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cider_instance = CIDER(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = medqa_test[122]\n",
    "query = f\"{test['question']}\\n\\n\" + \"Options: \" + \" \".join([f\"({key}) {value}\" for key, value in test['options'].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A 28-year-old gravida 1 at 32 weeks gestation is evaluated for an abnormal ultrasound that showed fetal microcephaly. Early in the 1st trimester, she had fevers and headaches for 1 week. She also experienced myalgias, arthralgias, and a pruritic maculopapular rash. The symptoms resolved without any medications. A week prior to her symptoms, she had traveled to Brazil where she spent most of the evenings hiking. She did not use any mosquito repellents. There is no personal or family history of chronic or congenital diseases. Medications include iron supplementation and a multivitamin. She received all of the recommended childhood vaccinations. She does not drink alcohol or smoke cigarettes. The IgM and IgG titers for toxoplasmosis were negative. Which of the following is the most likely etiologic agent?\\n\\nOptions: (A) Dengue virus (B) Rubella virus (C) Toxoplasmosis (D) Zika virus'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b91f4dca0940d59448a1633cd66b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Queries:   0%|          | 0/1273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "difficulty_list = [None] * len(medqa_test)  # Pre-allocate the list to maintain order\n",
    "\n",
    "def assess_difficulty(index, test):\n",
    "    query = f\"{test['question']}\\n\\n\" + \"Options: \" + \" \".join([f\"({key}) {value}\" for key, value in test['options'].items()])\n",
    "    return index, cider_instance.assess_query_difficulty(query)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(assess_difficulty, i, test) for i, test in enumerate(medqa_test)]\n",
    "    \n",
    "    for future in tqdm(futures, total=len(medqa_test), desc=\"Processing Queries\"):\n",
    "        index, result = future.result()\n",
    "        difficulty_list[index] = result  # Store the result at the correct index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficulty_list.count('easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_direct = []\n",
    "with open(\"/data/jiwoong/workspace/MedAgents-2/baselines/MedAgents/outputs/MedQA/gpt4omini-base_direct\", 'r') as jsfile:\n",
    "    for line in jsfile:\n",
    "        base_direct.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8368700265251989\n",
      "0.7341040462427746\n"
     ]
    }
   ],
   "source": [
    "easy=0\n",
    "hard=0\n",
    "for i in range(1273):\n",
    "    if difficulty_list[i] == 'easy' and base_direct[i]['pred_answer'] == base_direct[i]['gold_answer']:\n",
    "        easy += 1\n",
    "    elif difficulty_list[i] == 'hard' and base_direct[i]['pred_answer'] == base_direct[i]['gold_answer']:\n",
    "        hard += 1\n",
    "print(easy/difficulty_list.count('easy'))\n",
    "print(hard/difficulty_list.count('hard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
